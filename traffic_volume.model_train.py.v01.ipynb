{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Traffic Volume Prediction__\n",
    "<h2 align=\"center\"><b>Advanced Data Science Capstone Poject by</b></h2>\n",
    "<h2 align=\"center\"><b>IBM / Coursera</b></h1>\n",
    "<h2 align=center>Vasilis Kokkinos (September 2019)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Introduction / Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE CASE: Predictive model of traffic volume. It can be used as template for similar situations.\n",
    "\n",
    "DATA SET: Metro Interstate Traffic Volume Data Set\n",
    "Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN. Hourly weather features and holidays included for impacts on traffic volume.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Attribute Information:__\n",
    "\n",
    "__holiday:__ Categorical US National holidays plus regional holiday, Minnesota State Fair\n",
    "\n",
    "__temp:__ Numeric Average temp in kelvin\n",
    "\n",
    "__rain_1h:__ Numeric Amount in mm of rain that occurred in the hour\n",
    "\n",
    "__snow_1h:__ Numeric Amount in mm of snow that occurred in the hour\n",
    "\n",
    "__clouds_all:__ Numeric Percentage of cloud cover\n",
    "\n",
    "__weather_main:__ Categorical Short textual description of the current weather\n",
    "\n",
    "__weather_description:__ Categorical Longer textual description of the current weather\n",
    "\n",
    "__date_time:__ DateTime Hour of the data collected in local CST time\n",
    "\n",
    "__traffic_volume:__ Numeric Hourly I-94 ATR 301 reported westbound traffic volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Model Definition__ was performed in the previous step in the notebook __traffic_volume.model_def.py.v01.ipynb__\n",
    "\n",
    "In that step:\n",
    "* The initial data frame was split into three parts training, validation and testing. The initial data frame as well as the resulting three, all now have extra normalized columns to be used in deep learning and a vector column to be used in the non deep learning algorithm\n",
    "* It was determined that the problem was a __non-linear regression__ one\n",
    "* Initial algorithms were defined, both non-deep learning and deep learning\n",
    "\n",
    "The resulting data sets and alogorithms were saved and they will be used in the current notebook\n",
    "    \n",
    "This notebook, is the __Model Training__ step where the models will be trained and fine-tuned for better peerformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages, initialize Apache Spark session, and add supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.44.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Traffic Volume Prediction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2057f9fd888>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Traffic Volume Prediction').getOrCreate()\n",
    "\n",
    "# Create an sql context so that we can query data files in sql like syntax\n",
    "sqlContext = SQLContext(spark)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Read in the training data set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temp: double (nullable = true)\n",
      " |-- rain_1h: double (nullable = true)\n",
      " |-- snow_1h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- traffic_volume: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- holidayIndex: integer (nullable = true)\n",
      " |-- weatherMainIndex: integer (nullable = true)\n",
      " |-- weatherDescIndex: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_norm: vector (nullable = true)\n",
      " |-- temp_Norm: double (nullable = true)\n",
      " |-- rain_1h_Norm: double (nullable = true)\n",
      " |-- snow_1h_Norm: double (nullable = true)\n",
      " |-- clouds_all_Norm: double (nullable = true)\n",
      " |-- traffic_volume_Norm: double (nullable = true)\n",
      " |-- month_Norm: double (nullable = true)\n",
      " |-- day_of_week_Norm: double (nullable = true)\n",
      " |-- hour_of_day_Norm: double (nullable = true)\n",
      " |-- holidayIndex_Norm: double (nullable = true)\n",
      " |-- weatherMainIndex_Norm: double (nullable = true)\n",
      " |-- weatherDescIndex_Norm: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = spark.read.parquet('traffic_volume_df_train.parquet')\n",
    "\n",
    "df_train.createOrReplaceTempView('df_train')\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data set checks\n",
    "For each initial column of the training data frame (and the ones that will be used in this step), there is a corresponding normalized column with the suffix __'_NORM'__\n",
    "\n",
    "A vector column __'features_norm'__ will be used for the non deep learning algorithm. All other independent features will be used for the deep learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+----------+--------------+-----+-----------+-----------+------------+----------------+----------------+---------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+---------+------------+------------+---------------+-------------------+----------+----------------+----------------+-----------------+---------------------+---------------------+\n",
      "|temp  |rain_1h|snow_1h|clouds_all|traffic_volume|month|day_of_week|hour_of_day|holidayIndex|weatherMainIndex|weatherDescIndex|features                                     |features_norm                                                                                                             |temp_Norm|rain_1h_Norm|snow_1h_Norm|clouds_all_Norm|traffic_volume_Norm|month_Norm|day_of_week_Norm|hour_of_day_Norm|holidayIndex_Norm|weatherMainIndex_Norm|weatherDescIndex_Norm|\n",
      "+------+-------+-------+----------+--------------+-----+-----------+-----------+------------+----------------+----------------+---------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+---------+------------+------------+---------------+-------------------+----------+----------------+----------------+-----------------+---------------------+---------------------+\n",
      "|243.39|0.0    |0.0    |1         |1462          |12   |7          |8          |0           |5               |9               |[243.39,0.0,0.0,1.0,12.0,7.0,8.0,0.0,5.0,9.0]|[0.0,0.0,0.0,0.01,1.0,1.0,0.34782608695652173,0.0,0.5,0.2727272727272727]                                                 |0.0      |0.0         |0.0         |0.01           |0.2008242          |1.0       |1.0             |0.3478261       |0.0              |0.5                  |0.2727273            |\n",
      "|243.62|0.0    |0.0    |1         |1037          |12   |7          |7          |0           |5               |9               |[243.62,0.0,0.0,1.0,12.0,7.0,7.0,0.0,5.0,9.0]|[0.003449310137972678,0.0,0.0,0.01,1.0,1.0,0.30434782608695654,0.0,0.5,0.2727272727272727]                                |0.0034493|0.0         |0.0         |0.01           |0.1424451          |1.0       |1.0             |0.3043478       |0.0              |0.5                  |0.2727273            |\n",
      "|244.22|0.0    |0.0    |1         |800           |12   |7          |6          |0           |1               |0               |[244.22,0.0,0.0,1.0,12.0,7.0,6.0,0.0,1.0,0.0]|[0.012447510497900606,0.0,0.0,0.01,1.0,1.0,0.2608695652173913,0.0,0.1,0.0]                                                |0.0124475|0.0         |0.0         |0.01           |0.1098901          |1.0       |1.0             |0.2608696       |0.0              |0.1                  |0.0                  |\n",
      "|244.82|0.0    |0.0    |11        |354           |2    |6          |3          |0           |0               |6               |[244.82,0.0,0.0,11.0,2.0,6.0,3.0,0.0,0.0,6.0]|[0.021445710857828534,0.0,0.0,0.11,0.09090909090909091,0.8333333333333334,0.13043478260869565,0.0,0.0,0.18181818181818182]|0.0214457|0.0         |0.0         |0.11           |0.0486264          |0.0909091 |0.8333333       |0.1304348       |0.0              |0.0                  |0.1818182            |\n",
      "|244.89|0.0    |0.0    |1         |459           |12   |7          |5          |0           |1               |0               |[244.89,0.0,0.0,1.0,12.0,7.0,5.0,0.0,1.0,0.0]|[0.022495500899820033,0.0,0.0,0.01,1.0,1.0,0.21739130434782608,0.0,0.1,0.0]                                               |0.0224955|0.0         |0.0         |0.01           |0.0630495          |1.0       |1.0             |0.2173913       |0.0              |0.1                  |0.0                  |\n",
      "+------+-------+-------+----------+--------------+-----+-----------+-----------+------------+----------------+----------------+---------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+---------+------------+------------+---------------+-------------------+----------+----------------+----------------+-----------------+---------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the training dataframe: 28194\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows in the training dataframe: {}'.format(df_train.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __NON DEEP LEARNING MODEL TRAINING__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the pipeline for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestRegressor_931c203c4b3c]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_model_pl = Pipeline.load('rfr_model_pl.pkl')\n",
    "rfr_model_pl.getStages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline contains only one stage, a __RandomForestRegressor__. Let's extract the model and check its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor_931c203c4b3c__cacheNodeIds => False\n",
      "RandomForestRegressor_931c203c4b3c__checkpointInterval => 10\n",
      "RandomForestRegressor_931c203c4b3c__featureSubsetStrategy => auto\n",
      "RandomForestRegressor_931c203c4b3c__featuresCol => features_norm\n",
      "RandomForestRegressor_931c203c4b3c__impurity => variance\n",
      "RandomForestRegressor_931c203c4b3c__labelCol => traffic_volume\n",
      "RandomForestRegressor_931c203c4b3c__maxBins => 32\n",
      "RandomForestRegressor_931c203c4b3c__maxDepth => 5\n",
      "RandomForestRegressor_931c203c4b3c__maxMemoryInMB => 256\n",
      "RandomForestRegressor_931c203c4b3c__minInfoGain => 0.0\n",
      "RandomForestRegressor_931c203c4b3c__minInstancesPerNode => 1\n",
      "RandomForestRegressor_931c203c4b3c__numTrees => 20\n",
      "RandomForestRegressor_931c203c4b3c__predictionCol => prediction\n",
      "RandomForestRegressor_931c203c4b3c__seed => 6208668353818747712\n",
      "RandomForestRegressor_931c203c4b3c__subsamplingRate => 1.0\n"
     ]
    }
   ],
   "source": [
    "randomForestRegressor = rfr_model_pl.getStages()[-1]\n",
    "model_params = randomForestRegressor.extractParamMap()\n",
    "for param, value in model_params.items():\n",
    "    print(param, '=>', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model with the training data set and with its default parameters and also evaluate it against the training data set\n",
    "\n",
    "As performance measurement, I will use __R-squared (r2)__ so I can get a normalized score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator().setMetricName('r2').setPredictionCol(\"prediction\").setLabelCol(\"traffic_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tScore:  0.8513601641933375\n"
     ]
    }
   ],
   "source": [
    "ndl_model_rfr = randomForestRegressor.fit(df_train)\n",
    "ndl_prediction_rfr = ndl_model_rfr.transform(df_train)\n",
    "ndl_score_rfr = evaluator.evaluate(ndl_prediction_rfr)\n",
    "\n",
    "print('\\tScore: ', ndl_score_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following the suggestiong of the instructor __Romeo Kienzler__, let's also try __GBTRegressor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tScore:  0.9426269147471091\n"
     ]
    }
   ],
   "source": [
    "gbtRegressor = GBTRegressor(labelCol='traffic_volume', featuresCol='features_norm', predictionCol='prediction')\n",
    "\n",
    "ndl_model_gbt = gbtRegressor.fit(df_train)\n",
    "ndl_prediction_gbt = ndl_model_gbt.transform(df_train)\n",
    "ndl_score_gbt = evaluator.evaluate(ndl_prediction_gbt)\n",
    "\n",
    "print('\\tScore: ', ndl_score_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indeed it performed much better\n",
    "Let's check its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTRegressor_05786ef0b7d6__seed => 6462429939505428196\n",
      "GBTRegressor_05786ef0b7d6__predictionCol => prediction\n",
      "GBTRegressor_05786ef0b7d6__labelCol => traffic_volume\n",
      "GBTRegressor_05786ef0b7d6__featuresCol => features_norm\n",
      "GBTRegressor_05786ef0b7d6__maxDepth => 5\n",
      "GBTRegressor_05786ef0b7d6__maxBins => 32\n",
      "GBTRegressor_05786ef0b7d6__minInstancesPerNode => 1\n",
      "GBTRegressor_05786ef0b7d6__minInfoGain => 0.0\n",
      "GBTRegressor_05786ef0b7d6__maxMemoryInMB => 256\n",
      "GBTRegressor_05786ef0b7d6__cacheNodeIds => False\n",
      "GBTRegressor_05786ef0b7d6__subsamplingRate => 1.0\n",
      "GBTRegressor_05786ef0b7d6__checkpointInterval => 10\n",
      "GBTRegressor_05786ef0b7d6__lossType => squared\n",
      "GBTRegressor_05786ef0b7d6__maxIter => 20\n",
      "GBTRegressor_05786ef0b7d6__stepSize => 0.1\n",
      "GBTRegressor_05786ef0b7d6__impurity => variance\n",
      "GBTRegressor_05786ef0b7d6__featureSubsetStrategy => all\n"
     ]
    }
   ],
   "source": [
    "model_params = gbtRegressor.extractParamMap()\n",
    "for param, value in model_params.items():\n",
    "    print(param, '=>', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One comparable parameter is __featureSubsetStrategy__. Let's try training and evaluating the randomForestRegressor with featureSubsetStrategy set to 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tScore:  0.9141535520423105\n"
     ]
    }
   ],
   "source": [
    "randomForestRegressor.set(randomForestRegressor.getParam('featureSubsetStrategy'), 'all')\n",
    "\n",
    "ndl_model_rfr = randomForestRegressor.fit(df_train)\n",
    "ndl_prediction_rfr = ndl_model_rfr.transform(df_train)\n",
    "ndl_score_rfr = evaluator.evaluate(ndl_prediction_rfr)\n",
    "\n",
    "print('\\tScore: ', ndl_score_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of randomForestRegressor did improve, but still, it did not reach the performance of gbtRegressor.\n",
    "\n",
    "#### So we will use __GBTRegressor__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __GRID SEARCH__\n",
    "#### Using Grid Seach, let's try to improve the performance of the model\n",
    "We will iterate through values of the parameters __maxDepth__ with values in (5, 10, 15, 20) and __minInfoGain__ with values in (0, 0.3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFor maxDepth = 5, minInfoGain = 0 => score = 0.9426269147471091\n",
      "\tFor maxDepth = 5, minInfoGain = 0.3 => score = 0.9426269147471091\n",
      "\tFor maxDepth = 5, minInfoGain = 0.5 => score = 0.9426269147471091\n",
      "\tFor maxDepth = 10, minInfoGain = 0 => score = 0.9738647586109566\n",
      "\tFor maxDepth = 10, minInfoGain = 0.3 => score = 0.9738647586013968\n",
      "\tFor maxDepth = 10, minInfoGain = 0.5 => score = 0.9738647586013968\n",
      "\tFor maxDepth = 15, minInfoGain = 0 => score = 0.9940033559532351\n",
      "\tFor maxDepth = 15, minInfoGain = 0.3 => score = 0.9943949621579685\n",
      "\tFor maxDepth = 15, minInfoGain = 0.5 => score = 0.9943949611595072\n",
      "\tFor maxDepth = 20, minInfoGain = 0 => score = 0.9984686737637012\n",
      "\tFor maxDepth = 20, minInfoGain = 0.3 => score = 0.9980227792209548\n",
      "\tFor maxDepth = 20, minInfoGain = 0.5 => score = 0.9978958468264516\n",
      "Maximum score 0.9984686737637012 was achieved with maxDepth 20 and minInfoGain 0\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters lists\n",
    "maxDepths = [5, 10, 15, 20]\n",
    "minInfoGains = [0, 0.3, 0.5]\n",
    "\n",
    "score_max = 0\n",
    "\n",
    "# Run the Grid Search\n",
    "for maxDepth in maxDepths:\n",
    "    gbtRegressor.set(gbtRegressor.getParam('maxDepth'), maxDepth)\n",
    "    for minInfoGain in minInfoGains:\n",
    "        gbtRegressor.set(gbtRegressor.getParam('minInfoGain'), minInfoGain)\n",
    "        score = evaluator.evaluate(gbtRegressor.fit(df_train).transform(df_train))\n",
    "        print('\\tFor maxDepth = {}, minInfoGain = {} => score = {}'.format(maxDepth, minInfoGain, score))\n",
    "        if score > score_max:\n",
    "            score_max = score\n",
    "            maxDepth_max = maxDepth\n",
    "            minInfoGain_max = minInfoGain\n",
    "\n",
    "print('Maximum score {} was achieved with maxDepth {} and minInfoGain {}'.format(score_max, maxDepth_max, minInfoGain_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's just again define the model with __maxDepth 20__ and __minInfoGain 0__ (which is the default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtRegressor = GBTRegressor(labelCol='traffic_volume', featuresCol='features_norm', predictionCol='prediction', maxDepth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's create a Pipeline with the __GBTRegressor__ model as its only stage and save it in __gbt_model_pl.pkl__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_ndl_model_pl = Pipeline(stages=[gbtRegressor])\n",
    "gbt_ndl_model_pl.write().overwrite().save('gbt_model_pl.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __DEEP LEARNING MODEL TRAINING__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will work with the training data set __df_train__ and use the validation data set __df_val__ for initial validation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temp: double (nullable = true)\n",
      " |-- rain_1h: double (nullable = true)\n",
      " |-- snow_1h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- traffic_volume: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- holidayIndex: integer (nullable = true)\n",
      " |-- weatherMainIndex: integer (nullable = true)\n",
      " |-- weatherDescIndex: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_norm: vector (nullable = true)\n",
      " |-- temp_Norm: double (nullable = true)\n",
      " |-- rain_1h_Norm: double (nullable = true)\n",
      " |-- snow_1h_Norm: double (nullable = true)\n",
      " |-- clouds_all_Norm: double (nullable = true)\n",
      " |-- traffic_volume_Norm: double (nullable = true)\n",
      " |-- month_Norm: double (nullable = true)\n",
      " |-- day_of_week_Norm: double (nullable = true)\n",
      " |-- hour_of_day_Norm: double (nullable = true)\n",
      " |-- holidayIndex_Norm: double (nullable = true)\n",
      " |-- weatherMainIndex_Norm: double (nullable = true)\n",
      " |-- weatherDescIndex_Norm: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_val = spark.read.parquet('traffic_volume_df_val.parquet')\n",
    "\n",
    "df_val.createOrReplaceTempView('df_val')\n",
    "df_val.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+----------+--------------+-----+-----------+-----------+------------+----------------+----------------+---------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+---------+------------+------------+---------------+-------------------+----------+----------------+----------------+-----------------+---------------------+---------------------+\n",
      "|temp  |rain_1h|snow_1h|clouds_all|traffic_volume|month|day_of_week|hour_of_day|holidayIndex|weatherMainIndex|weatherDescIndex|features                                     |features_norm                                                                                                             |temp_Norm|rain_1h_Norm|snow_1h_Norm|clouds_all_Norm|traffic_volume_Norm|month_Norm|day_of_week_Norm|hour_of_day_Norm|holidayIndex_Norm|weatherMainIndex_Norm|weatherDescIndex_Norm|\n",
      "+------+-------+-------+----------+--------------+-----+-----------+-----------+------------+----------------+----------------+---------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+---------+------------+------------+---------------+-------------------+----------+----------------+----------------+-----------------+---------------------+---------------------+\n",
      "|244.82|0.0    |0.0    |11        |678           |2    |6          |5          |0           |0               |6               |[244.82,0.0,0.0,11.0,2.0,6.0,5.0,0.0,0.0,6.0]|[0.021445710857828534,0.0,0.0,0.11,0.09090909090909091,0.8333333333333334,0.21739130434782608,0.0,0.0,0.18181818181818182]|0.0214457|0.0         |0.0         |0.11           |0.0931319          |0.0909091 |0.8333333       |0.2173913       |0.0              |0.0                  |0.1818182            |\n",
      "+------+-------+-------+----------+--------------+-----+-----------+-----------+------------+----------------+----------------+---------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+---------+------------+------------+---------------+-------------------+----------+----------------+----------------+-----------------+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_val.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the validation dataframe: 8266\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows in the validation dataframe: {}'.format(df_val.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the normalized columns from both the training and the validation data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create arrays that correspond to the training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary dataframes\n",
    "X_train = df_train['temp_Norm', 'rain_1h_Norm', 'snow_1h_Norm', 'clouds_all_Norm', 'month_Norm', 'day_of_week_Norm',\n",
    "                'hour_of_day_Norm', 'holidayIndex_Norm', 'weatherMainIndex_Norm', 'weatherDescIndex_Norm']\n",
    "y_train = df_train[['traffic_volume_Norm']]\n",
    "X_val = df_val['temp_Norm', 'rain_1h_Norm', 'snow_1h_Norm', 'clouds_all_Norm', 'month_Norm', 'day_of_week_Norm',\n",
    "                'hour_of_day_Norm', 'holidayIndex_Norm', 'weatherMainIndex_Norm', 'weatherDescIndex_Norm']\n",
    "y_val = df_val[['traffic_volume_Norm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the corresponding arrays\n",
    "X_train.createOrReplaceTempView('X_train')\n",
    "X_train_arr = np.array(spark.sql('select * from X_train').collect())\n",
    "\n",
    "y_train.createOrReplaceTempView('y_train')\n",
    "y_train_arr = np.array(spark.sql('select * from y_train').collect())\n",
    "\n",
    "X_val.createOrReplaceTempView('X_val')\n",
    "X_val_arr = np.array(spark.sql('select * from X_val').collect())\n",
    "\n",
    "y_val.createOrReplaceTempView('y_val')\n",
    "y_val_arr = np.array(spark.sql('select * from y_val').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Keras__ is used as the Deep Learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.initializers import VarianceScaling\n",
    "\n",
    "# For the model compilation\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's import the architecture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               10752     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 188,829\n",
      "Trainable params: 188,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with open('traffic_volume_dl_model.json') as traffic_volume_dl_model_json_file:\n",
    "    traffic_volume_dl_model_json = json.load(traffic_volume_dl_model_json_file)\n",
    "model = model_from_json(traffic_volume_dl_model_json)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our initial mdel definition, the optimizer was __Adam__ and the activation on the last layer was __linear__\n",
    "\n",
    "Now we will run a __Grid Search__ to experiment wih optimizer __Nadam__ and activation __relu__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a create a function to define, compile and fit the model for different values of optimizers __(Adam, Nadam)__ and activations of the last model of the layer __(linear, relu)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel initializer will be from __VarianceScaling__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_initializer=VarianceScaling(distribution='uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pair of activation and optimizer, the model will be fit for 300 epochs in silent mode so that the output will not fill the notebook.\n",
    "\n",
    "At the end of each fit operation the model will be evaluated against the training data and the metrics will be displayed and finally the best result will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(activation, optimizer):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_shape=(10,), kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(512, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(256, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(128, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(64, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(32, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(64, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(16, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=kernel_initializer, activation=activation))\n",
    "\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "\n",
    "    model.fit(X_train_arr, y_train_arr, validation_data=(X_val_arr, y_val_arr), epochs=300, batch_size=127, verbose=0)\n",
    "\n",
    "    score = model.evaluate(X_train_arr, y_train_arr)\n",
    "    print('\\t\\tEvaluation Mean Squared Error: ', score[1])\n",
    "    print('\\t\\tEvaluation Mean Absolute Error: ', score[2])\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Grid Search__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation:  linear\n",
      "\tOptimizer:  Adam\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "28194/28194 [==============================] - 1s 41us/step\n",
      "\t\tEvaluation Mean Squared Error:  0.001726430579097349\n",
      "\t\tEvaluation Mean Absolute Error:  0.025934657405020965\n",
      "\tOptimizer:  Nadam\n",
      "28194/28194 [==============================] - 1s 43us/step\n",
      "\t\tEvaluation Mean Squared Error:  0.0016975082257168492\n",
      "\t\tEvaluation Mean Absolute Error:  0.02589232442272786\n",
      "Activation:  relu\n",
      "\tOptimizer:  Adam\n",
      "28194/28194 [==============================] - 1s 44us/step\n",
      "\t\tEvaluation Mean Squared Error:  0.0018787946306015133\n",
      "\t\tEvaluation Mean Absolute Error:  0.026447327834896697\n",
      "\tOptimizer:  Nadam\n",
      "28194/28194 [==============================] - 1s 46us/step\n",
      "\t\tEvaluation Mean Squared Error:  0.0020780536961155943\n",
      "\t\tEvaluation Mean Absolute Error:  0.02882944386128188\n",
      "\n",
      "Grid Search elaped time:  1856.796\n",
      "\n",
      "\n",
      "FINISHED GRID SEARCH\n"
     ]
    }
   ],
   "source": [
    "score_min = 1\n",
    "\n",
    "optimizers = ['Adam', 'Nadam']\n",
    "activations = ['linear', 'relu']\n",
    "\n",
    "optimizer_min = ''\n",
    "activation_min = ''\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for activation in activations:\n",
    "    print('Activation: ', activation)\n",
    "    for optimizer in optimizers:\n",
    "        print('\\tOptimizer: ', optimizer)\n",
    "        if optimizer == 'Adam':\n",
    "            optim = Adam()\n",
    "        elif optimizer == 'Nadam':\n",
    "            optim = Nadam()\n",
    "        score = test_model(activation, optim)\n",
    "        if score <= score_min:\n",
    "            score_min = score\n",
    "            activation_min = activation\n",
    "            optimizer_min = optimizer\n",
    "                                \n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('\\nGrid Search elaped time: ', round(elapsed, 3))\n",
    "print('\\n\\nFINISHED GRID SEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mean Squared Error: 0.0016975082257168492 with Activation linear and Optimizer Nadam:\n"
     ]
    }
   ],
   "source": [
    "print('Best Mean Squared Error: {} with Activation {} and Optimizer {}:'.format(score_min, activation_min, optimizer_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's again define our model using the best values for activation and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "optimizer = Nadam()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(10,), kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(512, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(256, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(128, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(64, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(32, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(64, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(16, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(4, kernel_initializer=kernel_initializer, activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer=kernel_initializer, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once more, let's fit and evaluate the model against the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28194/28194 [==============================] - 1s 52us/step\n",
      "\t\tEvaluation Mean Squared Error: 0.001674305948712752\n",
      "\t\tEvaluation Mean Absolute Error: 0.025758778695021065\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_arr, y_train_arr, validation_data=(X_val_arr, y_val_arr), epochs=300, batch_size=127, verbose=0)\n",
    "\n",
    "score = model.evaluate(X_train_arr, y_train_arr)\n",
    "print('\\t\\tEvaluation Mean Squared Error: {}'.format(score[1]))\n",
    "print('\\t\\tEvaluation Mean Absolute Error: {}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the __Model Evaluation__ step, we will evaluate our model against the validation and test data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now save the full model in .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('dl_full_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's also visualize how loss is decreased with the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Summary__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have defined the final non deep learning and deep leerning algorithm for our non-linear regression problem and saved them in:\n",
    "__gbt_model_pl.pkl__ and __dl_full_model.h5__\n",
    "\n",
    "The Pipeline and the non-linear model will be taken to the next steps where the algorithms will be evaluated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
