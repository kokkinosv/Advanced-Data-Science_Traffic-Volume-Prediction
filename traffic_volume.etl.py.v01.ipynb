{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Traffic Volume Prediction__\n",
    "<h2 align=\"center\"><b>Advanced Data Science Capstone Poject by</b></h2>\n",
    "<h2 align=\"center\"><b>IBM / Coursera</b></h1>\n",
    "<h2 align=center>Vasilis Kokkinos (September 2019)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Introduction / Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE CASE: Predictive model of traffic volume. It can be used as template for similar situations.\n",
    "\n",
    "DATA SET: Metro Interstate Traffic Volume Data Set\n",
    "Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN. Hourly weather features and holidays included for impacts on traffic volume.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Attribute Information:__\n",
    "\n",
    "__holiday:__ Categorical US National holidays plus regional holiday, Minnesota State Fair\n",
    "\n",
    "__temp:__ Numeric Average temp in kelvin\n",
    "\n",
    "__rain_1h:__ Numeric Amount in mm of rain that occurred in the hour\n",
    "\n",
    "__snow_1h:__ Numeric Amount in mm of snow that occurred in the hour\n",
    "\n",
    "__clouds_all:__ Numeric Percentage of cloud cover\n",
    "\n",
    "__weather_main:__ Categorical Short textual description of the current weather\n",
    "\n",
    "__weather_description:__ Categorical Longer textual description of the current weather\n",
    "\n",
    "__date_time:__ DateTime Hour of the data collected in local CST time\n",
    "\n",
    "__traffic_volume:__ Numeric Hourly I-94 ATR 301 reported westbound traffic volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __ETL - Data Cleansing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data set is only a .csv, no ETL data transformations are needed.\n",
    "\n",
    "An __Initial Data Exploration__ step was performed in the notebook: _traffic_volume.data_exp.py.v01.ipynb_\n",
    "\n",
    "The following observations were made:\n",
    "\n",
    "* There are duplicates in the data set.\n",
    "* The temperature column has an extreme outlier with value 0. The temperature is measured in Kelvin and absolute 0 is an impossible temperature (Celsius = Kelvin - 273.15)\n",
    "* The rain_1h column has an extreme outlier with value: 9831.3\n",
    "    \n",
    "In this ETL notebook I will fix the above issues. However, some data quality issues will be addressed in the Feature Creation / Transformation step in another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages, initialize Apache Spark session, and add supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.44.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Traffic Volume Prediction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17728efd1c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Traffic Volume Prediction').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an sql context so that we can query data sets in sql like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Define the data set schema__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, DoubleType,\n",
    "                               IntegerType, StringType, TimestampType)\n",
    "\n",
    "traffic_schema = StructType([StructField('holiday', StringType(), True),\n",
    "                            StructField('temp', DoubleType(), True),\n",
    "                            StructField('rain_1h', DoubleType(), True ),\n",
    "                            StructField('snow_1h', DoubleType(), True),\n",
    "                            StructField('clouds_all', IntegerType(), True),\n",
    "                            StructField('weather_main', StringType(), True  ),\n",
    "                            StructField('weather_description', StringType(), True ),\n",
    "                            StructField('date_time', TimestampType(), True ),\n",
    "                            StructField('traffic_volume', IntegerType(), True)\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Read in the data set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- holiday: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- rain_1h: double (nullable = true)\n",
      " |-- snow_1h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- traffic_volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Metro_Interstate_Traffic_Volume.csv',\n",
    "                     header = True, \n",
    "                     schema = traffic_schema)\n",
    "df.createOrReplaceTempView('df')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data set checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|holiday|temp  |rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time          |traffic_volume|\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|None   |288.28|0.0    |0.0    |40        |Clouds      |scattered clouds   |2012-10-02 09:00:00|5545          |\n",
      "|None   |289.36|0.0    |0.0    |75        |Clouds      |broken clouds      |2012-10-02 10:00:00|4516          |\n",
      "|None   |289.58|0.0    |0.0    |90        |Clouds      |overcast clouds    |2012-10-02 11:00:00|4767          |\n",
      "|None   |290.13|0.0    |0.0    |90        |Clouds      |overcast clouds    |2012-10-02 12:00:00|5026          |\n",
      "|None   |291.14|0.0    |0.0    |75        |Clouds      |broken clouds      |2012-10-02 13:00:00|4918          |\n",
      "|None   |291.72|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-02 14:00:00|5181          |\n",
      "|None   |293.17|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-02 15:00:00|5584          |\n",
      "|None   |293.86|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-02 16:00:00|6015          |\n",
      "|None   |294.14|0.0    |0.0    |20        |Clouds      |few clouds         |2012-10-02 17:00:00|5791          |\n",
      "|None   |293.1 |0.0    |0.0    |20        |Clouds      |few clouds         |2012-10-02 18:00:00|4770          |\n",
      "|None   |290.97|0.0    |0.0    |20        |Clouds      |few clouds         |2012-10-02 19:00:00|3539          |\n",
      "|None   |289.38|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-02 20:00:00|2784          |\n",
      "|None   |288.61|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-02 21:00:00|2361          |\n",
      "|None   |287.16|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-02 22:00:00|1529          |\n",
      "|None   |285.45|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-02 23:00:00|963           |\n",
      "|None   |284.63|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-03 00:00:00|506           |\n",
      "|None   |283.47|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-03 01:00:00|321           |\n",
      "|None   |281.18|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-03 02:00:00|273           |\n",
      "|None   |281.09|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-03 03:00:00|367           |\n",
      "|None   |279.53|0.0    |0.0    |1         |Clear       |sky is clear       |2012-10-03 04:00:00|814           |\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe: 48204\n"
     ]
    }
   ],
   "source": [
    "print('Number of entries in the dataframe: {}'.format(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null values in holiday: 0\n",
      "null values in temp: 0\n",
      "null values in rain_1h: 0\n",
      "null values in snow_1h: 0\n",
      "null values in clouds_all: 0\n",
      "null values in weather_main: 0\n",
      "null values in weather_description: 0\n",
      "null values in date_time: 0\n",
      "null values in traffic_volume: 0\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    nulls = spark.sql('select count(*) from df where ' + column + ' is null').first()[0]\n",
    "    print('null values in ' + column + ': ' + str(nulls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get basic statistics measurements of the initial numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+---------------------+-----------------+\n",
      "|summary|temp              |rain_1h            |snow_1h              |clouds_all       |\n",
      "+-------+------------------+-------------------+---------------------+-----------------+\n",
      "|count  |48204             |48204              |48204                |48204            |\n",
      "|mean   |281.2058703012135 |0.33426396149697535|2.2238818355323212E-4|49.36223135009543|\n",
      "|stddev |13.338231912676308|44.78913303693933  |0.008167611205361601 |39.01575046141368|\n",
      "|min    |0.0               |0.0                |0.0                  |0                |\n",
      "|max    |310.07            |9831.3             |0.51                 |100              |\n",
      "+-------+------------------+-------------------+---------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['temp', 'rain_1h', 'snow_1h', 'clouds_all']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __GENERAL COLUMN VALUES CHECK__\n",
    "On every non-numeric column I will check if the corresponding values. I am looking for values that are different (uppercase / lowercase, et.) but have the same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __'holiday'__ column was checked during the initial data exploration step. It was fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__'weather_main'__ column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|weather_main|count|\n",
      "+------------+-----+\n",
      "|Thunderstorm|1034 |\n",
      "|Drizzle     |1821 |\n",
      "|Fog         |912  |\n",
      "|Clear       |13391|\n",
      "|Smoke       |20   |\n",
      "|Squall      |4    |\n",
      "|Mist        |5950 |\n",
      "|Clouds      |15164|\n",
      "|Rain        |5672 |\n",
      "|Snow        |2876 |\n",
      "|Haze        |1360 |\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select weather_main, count(*) as count from df group by weather_main').show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__'weather_description'__ column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------------------+-----+\n",
      "|weather_main|weather_description                |count|\n",
      "+------------+-----------------------------------+-----+\n",
      "|Clear       |sky is clear                       |11665|\n",
      "|Clear       |Sky is Clear                       |1726 |\n",
      "|Clouds      |few clouds                         |1956 |\n",
      "|Clouds      |scattered clouds                   |3461 |\n",
      "|Clouds      |broken clouds                      |4666 |\n",
      "|Clouds      |overcast clouds                    |5081 |\n",
      "|Drizzle     |heavy intensity drizzle            |64   |\n",
      "|Drizzle     |shower drizzle                     |6    |\n",
      "|Drizzle     |light intensity drizzle            |1100 |\n",
      "|Drizzle     |drizzle                            |651  |\n",
      "|Fog         |fog                                |912  |\n",
      "|Haze        |haze                               |1360 |\n",
      "|Mist        |mist                               |5950 |\n",
      "|Rain        |proximity shower rain              |136  |\n",
      "|Rain        |light rain                         |3372 |\n",
      "|Rain        |freezing rain                      |2    |\n",
      "|Rain        |moderate rain                      |1664 |\n",
      "|Rain        |heavy intensity rain               |467  |\n",
      "|Rain        |very heavy rain                    |18   |\n",
      "|Rain        |light intensity shower rain        |13   |\n",
      "|Smoke       |smoke                              |20   |\n",
      "|Snow        |heavy snow                         |616  |\n",
      "|Snow        |snow                               |293  |\n",
      "|Snow        |light snow                         |1946 |\n",
      "|Snow        |light shower snow                  |11   |\n",
      "|Snow        |light rain and snow                |6    |\n",
      "|Snow        |shower snow                        |1    |\n",
      "|Snow        |sleet                              |3    |\n",
      "|Squall      |SQUALLS                            |4    |\n",
      "|Thunderstorm|proximity thunderstorm with rain   |52   |\n",
      "|Thunderstorm|thunderstorm with drizzle          |2    |\n",
      "|Thunderstorm|thunderstorm                       |125  |\n",
      "|Thunderstorm|thunderstorm with heavy rain       |63   |\n",
      "|Thunderstorm|proximity thunderstorm with drizzle|13   |\n",
      "|Thunderstorm|proximity thunderstorm             |673  |\n",
      "|Thunderstorm|thunderstorm with rain             |37   |\n",
      "|Thunderstorm|thunderstorm with light drizzle    |15   |\n",
      "|Thunderstorm|thunderstorm with light rain       |54   |\n",
      "+------------+-----------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select weather_main, weather_description, count(*) as count from df group by weather_main, \\\n",
    "           weather_description order by weather_main').show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 38 values for __'weather_description'__. One obvious value to be corrected is the __'Sky is clear'__ It will be set to __'sky is clear'__.\n",
    "I am not an expert in meteorology, so I will leave the other values as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('weather_description', F.when(df['weather_description']=='Sky is Clear', 'sky is clear').\\\n",
    "                   otherwise(df[\"weather_description\"]))\n",
    "\n",
    "# Refreshing the temp view of the dataframe\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure it was corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------------------+-----+\n",
      "|weather_main|weather_description                |count|\n",
      "+------------+-----------------------------------+-----+\n",
      "|Clear       |sky is clear                       |13391|\n",
      "|Clouds      |broken clouds                      |4666 |\n",
      "|Clouds      |few clouds                         |1956 |\n",
      "|Clouds      |scattered clouds                   |3461 |\n",
      "|Clouds      |overcast clouds                    |5081 |\n",
      "|Drizzle     |shower drizzle                     |6    |\n",
      "|Drizzle     |drizzle                            |651  |\n",
      "|Drizzle     |heavy intensity drizzle            |64   |\n",
      "|Drizzle     |light intensity drizzle            |1100 |\n",
      "|Fog         |fog                                |912  |\n",
      "|Haze        |haze                               |1360 |\n",
      "|Mist        |mist                               |5950 |\n",
      "|Rain        |light rain                         |3372 |\n",
      "|Rain        |very heavy rain                    |18   |\n",
      "|Rain        |moderate rain                      |1664 |\n",
      "|Rain        |proximity shower rain              |136  |\n",
      "|Rain        |freezing rain                      |2    |\n",
      "|Rain        |heavy intensity rain               |467  |\n",
      "|Rain        |light intensity shower rain        |13   |\n",
      "|Smoke       |smoke                              |20   |\n",
      "|Snow        |light snow                         |1946 |\n",
      "|Snow        |shower snow                        |1    |\n",
      "|Snow        |heavy snow                         |616  |\n",
      "|Snow        |light rain and snow                |6    |\n",
      "|Snow        |sleet                              |3    |\n",
      "|Snow        |light shower snow                  |11   |\n",
      "|Snow        |snow                               |293  |\n",
      "|Squall      |SQUALLS                            |4    |\n",
      "|Thunderstorm|thunderstorm                       |125  |\n",
      "|Thunderstorm|proximity thunderstorm with rain   |52   |\n",
      "|Thunderstorm|thunderstorm with drizzle          |2    |\n",
      "|Thunderstorm|thunderstorm with heavy rain       |63   |\n",
      "|Thunderstorm|thunderstorm with light rain       |54   |\n",
      "|Thunderstorm|proximity thunderstorm with drizzle|13   |\n",
      "|Thunderstorm|thunderstorm with light drizzle    |15   |\n",
      "|Thunderstorm|proximity thunderstorm             |673  |\n",
      "|Thunderstorm|thunderstorm with rain             |37   |\n",
      "+------------+-----------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select weather_main, weather_description, count(*) as count from df group by weather_main, weather_description order by weather_main').show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __OUTLIERS__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will deal with the outliers that were detected in the initial Data Exploration phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __temp__ outliers\n",
    "Check the row where the value in __'temp' is 0__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|holiday|temp|rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time          |traffic_volume|\n",
      "+-------+----+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 03:00:00|361           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 04:00:00|734           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 05:00:00|2557          |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 06:00:00|5150          |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 03:00:00|291           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 04:00:00|284           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 05:00:00|434           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 06:00:00|739           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 07:00:00|962           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 08:00:00|1670          |\n",
      "+-------+----+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from df where temp = 0').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we have duplicate rows for these date_time values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|holiday|temp|rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time          |traffic_volume|\n",
      "+-------+----+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 03:00:00|361           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 04:00:00|734           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 05:00:00|2557          |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 06:00:00|5150          |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 03:00:00|291           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 04:00:00|284           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 05:00:00|434           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 06:00:00|739           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 07:00:00|962           |\n",
      "|None   |0.0 |0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 08:00:00|1670          |\n",
      "+-------+----+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from df where date_time in (select date_time from df where temp = 0) order by date_time').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is not a case of duplicates. Probably something was wrong with the thermometer at these times? Maybe we could check the weather conditions __around__ these date_time values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|holiday|temp  |rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time          |traffic_volume|\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|None   |255.93|0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 02:00:00|313           |\n",
      "|None   |255.93|0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 07:00:00|5363          |\n",
      "|None   |255.37|0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 02:00:00|539           |\n",
      "|None   |255.62|0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 09:00:00|2506          |\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_time_str = '(\\'2014-01-31 02:00:00\\', \\'2014-01-31 07:00:00\\', \\'2014-02-02 02:00:00\\', \\'2014-02-02 09:00:00\\')'\n",
    "spark.sql('select * from df where date_time in ' + date_time_str).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually these values make sense, so:\n",
    "* for the 2014-01-31 outliers I will assign the temp value 255.93\n",
    "* for the 2014-02-02 outliers I will assign the temp value 255.37\n",
    "\n",
    "It is a calculated guess, and most possibly the new values are near the real ones of these dates and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|holiday|temp  |rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time          |traffic_volume|\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|None   |255.93|0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 02:00:00|313           |\n",
      "|None   |255.93|0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 04:00:00|734           |\n",
      "|None   |255.93|0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 05:00:00|2557          |\n",
      "|None   |255.93|0.0    |0.0    |0         |Clear       |sky is clear       |2014-01-31 06:00:00|5150          |\n",
      "|None   |255.37|0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 03:00:00|291           |\n",
      "|None   |255.37|0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 04:00:00|284           |\n",
      "|None   |255.37|0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 05:00:00|434           |\n",
      "|None   |255.37|0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 06:00:00|739           |\n",
      "|None   |255.37|0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 07:00:00|962           |\n",
      "|None   |255.37|0.0    |0.0    |0         |Clear       |sky is clear       |2014-02-02 08:00:00|1670          |\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-01-31 03:00:00', 255.93).otherwise(df[\"temp\"]))\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-01-31 04:00:00', 255.93).otherwise(df[\"temp\"]))\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-01-31 05:00:00', 255.93).otherwise(df[\"temp\"]))\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-01-31 06:00:00', 255.93).otherwise(df[\"temp\"]))\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-02-02 03:00:00', 255.37).otherwise(df[\"temp\"]))\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-02-02 04:00:00', 255.37).otherwise(df[\"temp\"]))\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-02-02 05:00:00', 255.37).otherwise(df[\"temp\"]))\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-02-02 06:00:00', 255.37).otherwise(df[\"temp\"]))\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-02-02 07:00:00', 255.37).otherwise(df[\"temp\"]))\n",
    "df = df.withColumn('temp', F.when(df['date_time']=='2014-02-02 08:00:00', 255.37).otherwise(df[\"temp\"]))\n",
    "\n",
    "# Refreshing the temp view of the dataframe\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "# And let's check the new values\n",
    "date_time_str = '(\\'2014-01-31 02:00:00\\', \\'2014-01-31 04:00:00\\', \\'2014-01-31 05:00:00\\', \\\n",
    "                  \\'2014-01-31 06:00:00\\', \\'2014-02-02 03:00:00\\', \\'2014-02-02 04:00:00\\',  \\\n",
    "                  \\'2014-02-02 05:00:00\\', \\'2014-02-02 06:00:00\\', \\'2014-02-02 06:00:00\\', \\\n",
    "                  \\'2014-02-02 07:00:00\\', \\'2014-02-02 08:00:00\\')'\n",
    "spark.sql('select * from df where date_time in ' + date_time_str).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __rain_1h__ outliers\n",
    "Check the row where the value in __'rain_1h' is 9831.3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|holiday|temp  |rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time          |traffic_volume|\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|None   |302.11|9831.3 |0.0    |75        |Rain        |very heavy rain    |2016-07-11 17:00:00|5535          |\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from df where rain_1h = 9831.3').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we could check the weather conditions __around__ these date_time value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+----------+------------+----------------------+-------------------+--------------+\n",
      "|holiday|temp  |rain_1h|snow_1h|clouds_all|weather_main|weather_description   |date_time          |traffic_volume|\n",
      "+-------+------+-------+-------+----------+------------+----------------------+-------------------+--------------+\n",
      "|None   |301.48|0.0    |0.0    |75        |Thunderstorm|proximity thunderstorm|2016-07-11 16:00:00|5934          |\n",
      "|None   |302.54|0.0    |0.0    |75        |Thunderstorm|proximity thunderstorm|2016-07-11 18:00:00|3900          |\n",
      "+-------+------+-------+-------+----------+------------+----------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_time_str = '(\\'2016-07-11 16:00:00\\', \\'2016-07-11 18:00:00\\')'\n",
    "spark.sql('select * from df where date_time in ' + date_time_str).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time I will __delete__ the outlier row. I don't have any indication of what the amount of rain was in that hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+-------+----------+------------+-------------------+---------+--------------+\n",
      "|holiday|temp|rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time|traffic_volume|\n",
      "+-------+----+-------+-------+----------+------------+-------------------+---------+--------------+\n",
      "+-------+----+-------+-------+----------+------------+-------------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(df['rain_1h'] < 9831.3)\n",
    "# Refreshing the temp view of the dataframe\n",
    "df.createOrReplaceTempView('df')\n",
    "date_time_str = '(\\'2016-07-11 17:00:00\\')'\n",
    "spark.sql('select * from df where date_time in ' + date_time_str).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Duplicate Rows Deletion__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the __'date_time'__ values should be unique in the data set I will base the check for duplicates on that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|date_time          |count|\n",
      "+-------------------+-----+\n",
      "|2013-04-18 22:00:00|6    |\n",
      "|2013-05-19 10:00:00|6    |\n",
      "|2016-03-27 04:00:00|6    |\n",
      "|2013-06-01 02:00:00|5    |\n",
      "|2018-04-13 21:00:00|5    |\n",
      "|2012-12-16 19:00:00|5    |\n",
      "|2013-04-11 01:00:00|5    |\n",
      "|2016-12-25 21:00:00|5    |\n",
      "|2012-10-25 15:00:00|5    |\n",
      "|2013-05-19 09:00:00|5    |\n",
      "|2012-10-26 04:00:00|5    |\n",
      "|2017-04-15 07:00:00|5    |\n",
      "|2013-12-03 13:00:00|5    |\n",
      "|2013-05-31 00:00:00|5    |\n",
      "|2013-05-20 17:00:00|5    |\n",
      "|2013-04-22 19:00:00|5    |\n",
      "|2018-04-14 09:00:00|5    |\n",
      "|2017-11-05 01:00:00|5    |\n",
      "|2013-04-18 23:00:00|5    |\n",
      "|2013-05-19 08:00:00|5    |\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select date_time, count(*) as count from df group by date_time having count > 1 order by count desc').show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, the rows where all the columns have duplicate values will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of initial distinct entries in the dataframe : 48186\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "print('Number of initial distinct entries in the dataframe : {}'.format(df.count()))\n",
    "\n",
    "# Refresh the view of the dataset\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|date_time          |count|\n",
      "+-------------------+-----+\n",
      "|2013-04-18 22:00:00|6    |\n",
      "|2013-05-19 10:00:00|6    |\n",
      "|2016-03-27 04:00:00|6    |\n",
      "|2013-06-01 02:00:00|5    |\n",
      "|2013-05-20 17:00:00|5    |\n",
      "|2012-12-16 19:00:00|5    |\n",
      "|2013-12-03 13:00:00|5    |\n",
      "|2018-04-13 21:00:00|5    |\n",
      "|2013-04-22 19:00:00|5    |\n",
      "|2018-09-20 18:00:00|5    |\n",
      "|2013-05-19 08:00:00|5    |\n",
      "|2013-04-11 01:00:00|5    |\n",
      "|2013-05-31 00:00:00|5    |\n",
      "|2016-12-25 21:00:00|5    |\n",
      "|2012-10-25 15:00:00|5    |\n",
      "|2018-04-14 09:00:00|5    |\n",
      "|2016-12-25 02:00:00|5    |\n",
      "|2013-04-18 23:00:00|5    |\n",
      "|2012-10-26 04:00:00|5    |\n",
      "|2017-04-15 07:00:00|5    |\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select date_time, count(*) as count from df group by date_time having count > 1 order by count desc').show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a couple of duplicate examples. Maybe we can identify a caterory of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+----------+------------+----------------------------+-------------------+--------------+\n",
      "|holiday|temp  |rain_1h|snow_1h|clouds_all|weather_main|weather_description         |date_time          |traffic_volume|\n",
      "+-------+------+-------+-------+----------+------------+----------------------------+-------------------+--------------+\n",
      "|None   |274.79|0.0    |0.0    |90        |Rain        |moderate rain               |2013-04-18 22:00:00|1532          |\n",
      "|None   |274.79|0.0    |0.0    |90        |Snow        |heavy snow                  |2013-04-18 22:00:00|1532          |\n",
      "|None   |274.79|0.0    |0.0    |90        |Mist        |mist                        |2013-04-18 22:00:00|1532          |\n",
      "|None   |274.79|0.0    |0.0    |90        |Snow        |snow                        |2013-04-18 22:00:00|1532          |\n",
      "|None   |274.79|0.0    |0.0    |90        |Drizzle     |light intensity drizzle     |2013-04-18 22:00:00|1532          |\n",
      "|None   |274.79|0.0    |0.0    |90        |Rain        |light rain                  |2013-04-18 22:00:00|1532          |\n",
      "|None   |287.15|0.0    |0.0    |90        |Rain        |moderate rain               |2013-05-19 10:00:00|3591          |\n",
      "|None   |287.15|0.0    |0.0    |90        |Thunderstorm|thunderstorm with heavy rain|2013-05-19 10:00:00|3591          |\n",
      "|None   |287.15|0.0    |0.0    |90        |Thunderstorm|thunderstorm with light rain|2013-05-19 10:00:00|3591          |\n",
      "|None   |287.15|0.0    |0.0    |90        |Rain        |light rain                  |2013-05-19 10:00:00|3591          |\n",
      "|None   |287.15|0.0    |0.0    |90        |Thunderstorm|proximity thunderstorm      |2013-05-19 10:00:00|3591          |\n",
      "|None   |287.15|0.0    |0.0    |90        |Mist        |mist                        |2013-05-19 10:00:00|3591          |\n",
      "+-------+------+-------+-------+----------+------------+----------------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from df where date_time in (\\'2013-04-18 22:00:00\\', \\'2013-05-19 10:00:00\\') order by date_time').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the colums weather_main and weather_description are responsible for the duplicates\n",
    "\n",
    "Let's try and group by those columns too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------------+-----+\n",
      "|date_time          |weather_main|weather_description|count|\n",
      "+-------------------+------------+-------------------+-----+\n",
      "|2012-11-04 01:00:00|Clouds      |overcast clouds    |2    |\n",
      "|2014-01-03 20:00:00|Clouds      |overcast clouds    |2    |\n",
      "|2014-01-16 16:00:00|Clouds      |broken clouds      |2    |\n",
      "|2014-01-19 16:00:00|Rain        |light rain         |2    |\n",
      "|2014-01-20 16:00:00|Clouds      |broken clouds      |2    |\n",
      "|2014-02-25 02:00:00|Clear       |sky is clear       |2    |\n",
      "|2014-02-25 03:00:00|Clear       |sky is clear       |2    |\n",
      "|2014-04-21 15:00:00|Clouds      |few clouds         |2    |\n",
      "|2014-05-10 04:00:00|Clear       |sky is clear       |2    |\n",
      "|2014-05-31 18:00:00|Rain        |light rain         |2    |\n",
      "|2014-06-01 05:00:00|Rain        |moderate rain      |2    |\n",
      "|2014-06-01 05:00:00|Thunderstorm|thunderstorm       |2    |\n",
      "|2014-07-06 07:00:00|Rain        |moderate rain      |2    |\n",
      "|2014-07-06 07:00:00|Mist        |mist               |2    |\n",
      "|2016-03-27 04:00:00|Rain        |moderate rain      |2    |\n",
      "|2016-03-27 04:00:00|Mist        |mist               |2    |\n",
      "|2016-03-27 04:00:00|Drizzle     |drizzle            |2    |\n",
      "|2016-05-24 10:00:00|Mist        |mist               |2    |\n",
      "|2016-05-24 14:00:00|Clouds      |scattered clouds   |2    |\n",
      "|2016-05-24 18:00:00|Haze        |haze               |2    |\n",
      "+-------------------+------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select date_time, weather_main, weather_description, count(*) as count \\\n",
    "          from df group by date_time, weather_main, weather_description having count > 1 \\\n",
    "          order by date_time').show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a couple of examples. Maybe we can identify a caterory of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|holiday|temp  |rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time          |traffic_volume|\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "|None   |267.97|0.0    |0.0    |90        |Clouds      |overcast clouds    |2014-01-03 20:00:00|2427          |\n",
      "|None   |268.88|0.0    |0.0    |90        |Clouds      |overcast clouds    |2014-01-03 20:00:00|2427          |\n",
      "|None   |265.22|0.0    |0.0    |76        |Snow        |light snow         |2016-12-07 07:00:00|5551          |\n",
      "|None   |265.22|0.0    |0.0    |90        |Snow        |light snow         |2016-12-07 07:00:00|5551          |\n",
      "+-------+------+-------+-------+----------+------------+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from df where date_time in (\\'2016-12-07 07:00:00\\', \\'2014-01-03 20:00:00\\') order by date_time').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that a lot of columns may contribute to duplicate date times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to only keep the first row for each 'date_time' that appears multple times in the data set, but it was extremely slow. In fact I had to interrupt the kernel before the operation finished.\n",
    "\n",
    "So I will remove all the duplicates from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40569"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropDuplicates(['date_time'])\n",
    "\n",
    "# Refreshing the temp view of the dataframe\n",
    "df.createOrReplaceTempView('df')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the duplicates were actually removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|date_time|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select date_time, count(*) as count from df group by date_time having count > 1 order by count desc').show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, let's get count of rows per year. We will also see the years we are talking about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|year|count(1)|\n",
      "+----+--------+\n",
      "|2012|    2103|\n",
      "|2013|    7293|\n",
      "|2014|    4500|\n",
      "|2015|    3593|\n",
      "|2016|    7836|\n",
      "|2017|    8712|\n",
      "|2018|    6532|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select year(date_time) as year, count(*) from df group by year order by year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to have about __8760__ (24 * 365) data points for each year.\n",
    "\n",
    "However there are a lot of missing values!!! \n",
    "\n",
    "Since there is no way to get the measurements needed, I will work with the available data.\n",
    "\n",
    "The missing data could affect the quality of the machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check that each holiday only appears once each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------+\n",
      "|year|             holiday|count(1)|\n",
      "+----+--------------------+--------+\n",
      "|2012|       Christmas Day|       1|\n",
      "|2012|        Columbus Day|       1|\n",
      "|2012|                None|    2099|\n",
      "|2012|    Thanksgiving Day|       1|\n",
      "|2012|        Veterans Day|       1|\n",
      "|2013|       Christmas Day|       1|\n",
      "|2013|        Columbus Day|       1|\n",
      "|2013|    Independence Day|       1|\n",
      "|2013|           Labor Day|       1|\n",
      "|2013|        Memorial Day|       1|\n",
      "|2013|       New Years Day|       1|\n",
      "|2013|                None|    7283|\n",
      "|2013|          State Fair|       1|\n",
      "|2013|    Thanksgiving Day|       1|\n",
      "|2013|        Veterans Day|       1|\n",
      "|2013|Washingtons Birthday|       1|\n",
      "|2014|Martin Luther Kin...|       1|\n",
      "|2014|        Memorial Day|       1|\n",
      "|2014|       New Years Day|       1|\n",
      "|2014|                None|    4496|\n",
      "|2014|Washingtons Birthday|       1|\n",
      "|2015|       Christmas Day|       1|\n",
      "|2015|        Columbus Day|       1|\n",
      "|2015|    Independence Day|       1|\n",
      "|2015|           Labor Day|       1|\n",
      "|2015|                None|    3586|\n",
      "|2015|          State Fair|       1|\n",
      "|2015|    Thanksgiving Day|       1|\n",
      "|2015|        Veterans Day|       1|\n",
      "|2016|       Christmas Day|       1|\n",
      "|2016|        Columbus Day|       1|\n",
      "|2016|    Independence Day|       1|\n",
      "|2016|           Labor Day|       1|\n",
      "|2016|        Memorial Day|       1|\n",
      "|2016|       New Years Day|       1|\n",
      "|2016|                None|    7826|\n",
      "|2016|          State Fair|       1|\n",
      "|2016|    Thanksgiving Day|       1|\n",
      "|2016|        Veterans Day|       1|\n",
      "|2016|Washingtons Birthday|       1|\n",
      "|2017|       Christmas Day|       1|\n",
      "|2017|        Columbus Day|       1|\n",
      "|2017|    Independence Day|       1|\n",
      "|2017|           Labor Day|       1|\n",
      "|2017|Martin Luther Kin...|       1|\n",
      "|2017|        Memorial Day|       1|\n",
      "|2017|       New Years Day|       1|\n",
      "|2017|                None|    8701|\n",
      "|2017|          State Fair|       1|\n",
      "|2017|    Thanksgiving Day|       1|\n",
      "|2017|        Veterans Day|       1|\n",
      "|2017|Washingtons Birthday|       1|\n",
      "|2018|    Independence Day|       1|\n",
      "|2018|           Labor Day|       1|\n",
      "|2018|Martin Luther Kin...|       1|\n",
      "|2018|        Memorial Day|       1|\n",
      "|2018|       New Years Day|       1|\n",
      "|2018|                None|    6525|\n",
      "|2018|          State Fair|       1|\n",
      "|2018|Washingtons Birthday|       1|\n",
      "+----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select year(date_time) as year, holiday, count(*) from df group by year, holiday order by year, holiday').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's again get basic statistics measurements of the initial numerical columns again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+---------------------+-----------------+\n",
      "|summary|temp              |rain_1h            |snow_1h              |clouds_all       |\n",
      "+-------+------------------+-------------------+---------------------+-----------------+\n",
      "|count  |40569             |40569              |40569                |40569            |\n",
      "|mean   |281.3785764007    |0.07635041534176344|1.1733096699450319E-4|44.19882176045749|\n",
      "|stddev |13.099580934750465|0.7696957215092919 |0.005676571537482932 |38.68463638299327|\n",
      "|min    |243.39            |0.0                |0.0                  |0                |\n",
      "|max    |310.07            |55.63              |0.51                 |100              |\n",
      "+-------+------------------+-------------------+---------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['temp', 'rain_1h', 'snow_1h', 'clouds_all']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the resulting data frame in __parquet__ format so that it can be read and used in the next phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1)\n",
    "df.write.parquet('traffic_volume_etl_df.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
